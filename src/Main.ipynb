{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/amatis/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/amatis/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# read all data file\n",
    "filename = \"../files/cacm.all_processed\"\n",
    "\n",
    "with open(filename, 'r') as file:\n",
    "    linesArr = file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# remove newlines\n",
    "for i in range(len(linesArr)):\n",
    "    linesArr[i] = linesArr[i].replace(\"\\n\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All documents have been successfully seperated.\n"
     ]
    }
   ],
   "source": [
    "# create array for each document\n",
    "documentsArr = []\n",
    "document = []\n",
    "for i in range(len(linesArr)):\n",
    "    line = linesArr[i]\n",
    "    if(line == \"SPLITTER_TEXT\"):\n",
    "        if(i != 0):\n",
    "            documentsArr.append(document)    \n",
    "        document = []\n",
    "    else:\n",
    "        document.append(line)\n",
    "        \n",
    "if(len(documentsArr) == 3204):\n",
    "    print(\"All documents have been successfully seperated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All documents have been successfully parsed.\n"
     ]
    }
   ],
   "source": [
    "documents = {}\n",
    "\n",
    "for document in documentsArr:\n",
    "    docObj = {}\n",
    "    \n",
    "    docID = int(document[0].split(\" \")[1])\n",
    "    \n",
    "    title = \"\"\n",
    "    \n",
    "    indexofT = document.index(\".T\")\n",
    "    indexofB = document.index(\".B\")\n",
    "    \n",
    "    if \".W\" in document:\n",
    "        abstract = \"\"\n",
    "        indexofW = document.index(\".W\")\n",
    "        for i in range(indexofT + 1, indexofW):\n",
    "            title += \" \" + document[i]\n",
    "        for i in range(indexofW + 1, indexofB):\n",
    "            abstract += \" \" + document[i]\n",
    "        docObj[\"abstract\"] = abstract[1:]\n",
    "        \n",
    "    else:\n",
    "        for i in range(indexofT + 1, indexofB):\n",
    "            title += \" \" + document[i]\n",
    "            \n",
    "    docObj[\"title\"] = title[1:]        \n",
    "            \n",
    "    if \".A\" in document:\n",
    "        authors = \"\"\n",
    "        indexofA = document.index(\".A\")\n",
    "        \n",
    "        if \".K\" in document:\n",
    "            indexofK = document.index(\".K\")\n",
    "            for i in range(indexofA + 1, indexofK):\n",
    "                authors += \" \" + document[i]\n",
    "                \n",
    "        else:\n",
    "            for i in range(indexofA + 1, len(document)):\n",
    "                authors += \" \" + document[i]\n",
    "            \n",
    "        docObj[\"authors\"] = authors[1:]\n",
    "        \n",
    "    if \".K\" in document:\n",
    "        keywords = \"\"\n",
    "        indexofK = document.index(\".K\")\n",
    "        for i in range(indexofK + 1, len(document)):\n",
    "            keywords += \" \" + document[i]\n",
    "        docObj[\"keywords\"] = keywords[1:]\n",
    "    \n",
    "\n",
    "    \n",
    "    documents[docID] = docObj\n",
    "\n",
    "if(len(documents) == 3204):\n",
    "    print(\"All documents have been successfully parsed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# read queries file\n",
    "filename = \"../files/query.text\"\n",
    "\n",
    "with open(filename, 'r') as file:\n",
    "    linesArr = file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# remove newlines\n",
    "for i in range(len(linesArr)):\n",
    "    linesArr[i] = linesArr[i].replace(\"\\n\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All queries have been successfully seperated.\n"
     ]
    }
   ],
   "source": [
    "# create array for each query\n",
    "queriesArr = []\n",
    "query = []\n",
    "for i in range(len(linesArr)):\n",
    "    line = linesArr[i]\n",
    "    if(line == \" \" or line == \"\"):\n",
    "        if(i != 0):\n",
    "            queriesArr.append(query)    \n",
    "        query = []\n",
    "    else:\n",
    "        query.append(line)\n",
    "        \n",
    "if(len(queriesArr) == 64):\n",
    "    print(\"All queries have been successfully seperated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All queries have been successfully parsed.\n"
     ]
    }
   ],
   "source": [
    "# additional parsing and organization of queries\n",
    "queries = {}\n",
    "\n",
    "for query in queriesArr:\n",
    "    queryText = \"\"\n",
    "    queryID = int(query[0].split(\" \")[1])\n",
    "    indexofW = query.index(\".W\")\n",
    "    if \".A\" in query:\n",
    "        indexofA = query.index(\".A\")\n",
    "        for i in range(indexofW + 1, indexofA):\n",
    "            queryText += \" \" + query[i]\n",
    "    else:\n",
    "        indexofN = query.index(\".N\")\n",
    "        for i in range(indexofW + 1, indexofN):\n",
    "            queryText += \" \" + query[i]\n",
    "    queries[queryID] = queryText[2:]\n",
    "    \n",
    "if(len(queries) == 64):\n",
    "    print(\"All queries have been successfully parsed.\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# read query relations file\n",
    "filename = \"../files/qrels.text\"\n",
    "\n",
    "with open(filename, 'r') as file:\n",
    "    linesArr = file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All query relations have been successfully parsed.\n"
     ]
    }
   ],
   "source": [
    "# create dictionary to store query relations\n",
    "queryRelations = {}\n",
    "\n",
    "for line in linesArr:\n",
    "    line = line.split(\" \")\n",
    "    queryID = int(line[0])\n",
    "    docID = int(line[1])\n",
    "    \n",
    "    if queryID in queryRelations:\n",
    "        temp = queryRelations[queryID]\n",
    "        temp.append(docID)\n",
    "        queryRelations[queryID] = temp\n",
    "    else:\n",
    "        queryRelations[queryID] = [docID]\n",
    "\n",
    "if(len(queryRelations) == 52):\n",
    "    print(\"All query relations have been successfully parsed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All common words have been successfully parsed.\n"
     ]
    }
   ],
   "source": [
    "# read query relations file\n",
    "filename = \"../files/common_words\"\n",
    "\n",
    "with open(filename, 'r') as file:\n",
    "    common_words = file.readlines()\n",
    "\n",
    "if len(common_words) == 429:\n",
    "    print(\"All common words have been successfully parsed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define sub-preprocessing functions\n",
    "def convert_to_lowercase(text):\n",
    "    return np.array_str(np.char.lower(text))\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    symbols = \"!\\\"#$%&()*+-./:;,<=>?@[\\]^_`{|}~\\n\"\n",
    "    for i in symbols:\n",
    "        text = np.array_str(np.char.replace(text, i, ' '))\n",
    "    return text  \n",
    "\n",
    "def remove_single_characters(text):\n",
    "    new_text = \"\"\n",
    "    words = text.split()\n",
    "    for w in words:\n",
    "        if len(w) > 1:\n",
    "           new_text += \" \" + w\n",
    "    if new_text[0] == \" \":\n",
    "        new_text = new_text[1:]\n",
    "        \n",
    "    return new_text\n",
    "\n",
    "def stem_words(text):\n",
    "    stemmer = nltk.stem.PorterStemmer()\n",
    "    new_text = \"\"\n",
    "    words = text.split()\n",
    "    for w in words:\n",
    "        w = stemmer.stem(w) \n",
    "        new_text += \" \" + w\n",
    "        \n",
    "    if new_text[0] == \" \":\n",
    "        new_text = new_text[1:]\n",
    "        \n",
    "    return new_text\n",
    "\n",
    "def lemmatize_words(text):\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    new_text = \"\"\n",
    "    words = text.split()\n",
    "    for w in words:\n",
    "        w = lemmatizer.lemmatize(w) \n",
    "        new_text += \" \" + w\n",
    "        \n",
    "    if new_text[0] == \" \":\n",
    "        new_text = new_text[1:]\n",
    "        \n",
    "    return new_text\n",
    "\n",
    "def remove_stopwords(text, choice):\n",
    "    stopword_list = []\n",
    "    if choice == \"own\":\n",
    "        stopword_list = common_words\n",
    "    elif choice == \"nltk\":\n",
    "        stopword_list = nltk.corpus.stopwords.words(\"english\")\n",
    "    else:\n",
    "        print(\"Choose 'own' or 'nltk'.\")\n",
    "        return text\n",
    "        \n",
    "    new_text = \"\"\n",
    "    words = text.split()\n",
    "    for w in words:\n",
    "        if w not in stopword_list:\n",
    "           new_text += \" \" + w\n",
    "    if new_text[0] == \" \":\n",
    "        new_text = new_text[1:]\n",
    "        \n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main preprocessing function\n",
    "def preprocess(text):\n",
    "    text = convert_to_lowercase(text)\n",
    "    text = remove_punctuation(text)\n",
    "    text = remove_single_characters(text)\n",
    "    \n",
    "    text = remove_stopwords(text, \"nltk\")\n",
    "    #text = remove_stopwords(text, \"own\")\n",
    "    \n",
    "    text = lemmatize_words(text)\n",
    "    #text = stem_words(text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'binary number system offer many advantage decimal representation high performance general purpose computer greater simplicity binary arithmetic unit greater compactness binary number contribute directly arithmetic speed le obvious perhaps important way binary addressing instruction format increase overall performance binary address also essential certain powerful operation practical decimal instruction format hand decimal number essential communicating man computer application requiring processing large volume inherently decimal input output data time decimal binary conversion needed purely binary computer may significant slower decimal adder may take le time fast binary adder addition two conversion careful review significance decimal binary addressing binary decimal data arithmetic supplemented efficient conversion instruction'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = documents[40][\"abstract\"]\n",
    "preprocess(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
