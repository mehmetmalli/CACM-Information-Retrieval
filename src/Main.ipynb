{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: pip is being invoked by an old script wrapper. This will fail in a future version of pip.\n",
      "Please see https://github.com/pypa/pip/issues/5599 for advice on fixing the underlying issue.\n",
      "To avoid this problem you can invoke Python with '-m pip' instead of running pip directly.\n",
      "Requirement already satisfied: num2words in /Users/blackcherry/anaconda3/lib/python3.6/site-packages (0.5.10)\n",
      "Requirement already satisfied: docopt>=0.6.2 in /Users/blackcherry/anaconda3/lib/python3.6/site-packages (from num2words) (0.6.2)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install num2words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/blackcherry/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/blackcherry/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "from num2words import num2words\n",
    "from collections import Counter\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# read all data file\n",
    "filename = \"../files/cacm.all_processed\"\n",
    "\n",
    "with open(filename, 'r') as file:\n",
    "    linesArr = file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# remove newlines\n",
    "for i in range(len(linesArr)):\n",
    "    linesArr[i] = linesArr[i].replace(\"\\n\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All documents have been successfully seperated.\n"
     ]
    }
   ],
   "source": [
    "# create array for each document\n",
    "documentsArr = []\n",
    "document = []\n",
    "for i in range(len(linesArr)):\n",
    "    line = linesArr[i]\n",
    "    if(line == \"SPLITTER_TEXT\"):\n",
    "        if(i != 0):\n",
    "            documentsArr.append(document)    \n",
    "        document = []\n",
    "    else:\n",
    "        document.append(line)\n",
    "        \n",
    "if(len(documentsArr) == 3204):\n",
    "    print(\"All documents have been successfully seperated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All documents have been successfully parsed.\n"
     ]
    }
   ],
   "source": [
    "documents = {}\n",
    "\n",
    "for document in documentsArr:\n",
    "    docObj = {}\n",
    "    \n",
    "    docID = int(document[0].split(\" \")[1])\n",
    "    \n",
    "    title = \"\"\n",
    "    \n",
    "    indexofT = document.index(\".T\")\n",
    "    indexofB = document.index(\".B\")\n",
    "    \n",
    "    if \".W\" in document:\n",
    "        abstract = \"\"\n",
    "        indexofW = document.index(\".W\")\n",
    "        for i in range(indexofT + 1, indexofW):\n",
    "            title += \" \" + document[i]\n",
    "        for i in range(indexofW + 1, indexofB):\n",
    "            abstract += \" \" + document[i]\n",
    "        docObj[\"abstract\"] = abstract[1:]\n",
    "        \n",
    "    else:\n",
    "        for i in range(indexofT + 1, indexofB):\n",
    "            title += \" \" + document[i]\n",
    "            \n",
    "    docObj[\"title\"] = title[1:]        \n",
    "            \n",
    "    if \".A\" in document:\n",
    "        authors = \"\"\n",
    "        indexofA = document.index(\".A\")\n",
    "        \n",
    "        if \".K\" in document:\n",
    "            indexofK = document.index(\".K\")\n",
    "            for i in range(indexofA + 1, indexofK):\n",
    "                authors += \" \" + document[i]\n",
    "                \n",
    "        else:\n",
    "            for i in range(indexofA + 1, len(document)):\n",
    "                authors += \" \" + document[i]\n",
    "            \n",
    "        docObj[\"authors\"] = authors[1:]\n",
    "        \n",
    "    if \".K\" in document:\n",
    "        keywords = \"\"\n",
    "        indexofK = document.index(\".K\")\n",
    "        for i in range(indexofK + 1, len(document)):\n",
    "            keywords += \" \" + document[i]\n",
    "        docObj[\"keywords\"] = keywords[1:]\n",
    "    \n",
    "\n",
    "    \n",
    "    documents[docID] = docObj\n",
    "\n",
    "if(len(documents) == 3204):\n",
    "    print(\"All documents have been successfully parsed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# read queries file\n",
    "filename = \"../files/query.text\"\n",
    "\n",
    "with open(filename, 'r') as file:\n",
    "    linesArr = file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# remove newlines\n",
    "for i in range(len(linesArr)):\n",
    "    linesArr[i] = linesArr[i].replace(\"\\n\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All queries have been successfully seperated.\n"
     ]
    }
   ],
   "source": [
    "# create array for each query\n",
    "queriesArr = []\n",
    "query = []\n",
    "for i in range(len(linesArr)):\n",
    "    line = linesArr[i]\n",
    "    if(line == \" \" or line == \"\"):\n",
    "        if(i != 0):\n",
    "            queriesArr.append(query)    \n",
    "        query = []\n",
    "    else:\n",
    "        query.append(line)\n",
    "        \n",
    "if(len(queriesArr) == 64):\n",
    "    print(\"All queries have been successfully seperated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All queries have been successfully parsed.\n"
     ]
    }
   ],
   "source": [
    "# additional parsing and organization of queries\n",
    "queries = {}\n",
    "\n",
    "for query in queriesArr:\n",
    "    queryText = \"\"\n",
    "    queryID = int(query[0].split(\" \")[1])\n",
    "    indexofW = query.index(\".W\")\n",
    "    if \".A\" in query:\n",
    "        indexofA = query.index(\".A\")\n",
    "        for i in range(indexofW + 1, indexofA):\n",
    "            queryText += \" \" + query[i]\n",
    "    else:\n",
    "        indexofN = query.index(\".N\")\n",
    "        for i in range(indexofW + 1, indexofN):\n",
    "            queryText += \" \" + query[i]\n",
    "    queries[queryID] = queryText[2:]\n",
    "    \n",
    "if(len(queries) == 64):\n",
    "    print(\"All queries have been successfully parsed.\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# read query relations file\n",
    "filename = \"../files/qrels.text\"\n",
    "\n",
    "with open(filename, 'r') as file:\n",
    "    linesArr = file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All query relations have been successfully parsed.\n"
     ]
    }
   ],
   "source": [
    "# create dictionary to store query relations\n",
    "queryRelations = {}\n",
    "\n",
    "for line in linesArr:\n",
    "    line = line.split(\" \")\n",
    "    queryID = int(line[0])\n",
    "    docID = int(line[1])\n",
    "    \n",
    "    if queryID in queryRelations:\n",
    "        temp = queryRelations[queryID]\n",
    "        temp.append(docID)\n",
    "        queryRelations[queryID] = temp\n",
    "    else:\n",
    "        queryRelations[queryID] = [docID]\n",
    "\n",
    "if(len(queryRelations) == 52):\n",
    "    print(\"All query relations have been successfully parsed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All common words have been successfully parsed.\n"
     ]
    }
   ],
   "source": [
    "# read query relations file\n",
    "filename = \"../files/common_words\"\n",
    "\n",
    "with open(filename, 'r') as file:\n",
    "    common_words = file.readlines()\n",
    "\n",
    "if len(common_words) == 429:\n",
    "    print(\"All common words have been successfully parsed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# define sub-preprocessing functions\n",
    "def convert_to_lowercase(text):\n",
    "    return np.array_str(np.char.lower(text))\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    symbols = \"!\\\"#$%&()*+-./:;,<=>?@[\\]^_`{|}~\\n\"\n",
    "    for i in symbols:\n",
    "        text = np.array_str(np.char.replace(text, i, ' '))\n",
    "    return text  \n",
    "\n",
    "def remove_apostrophe(text):\n",
    "    return np.array_str(np.char.replace(text, \"'\", \"\"))\n",
    "\n",
    "def remove_single_characters(text):\n",
    "    new_text = \"\"\n",
    "    words = text.split()\n",
    "    for w in words:\n",
    "        if len(w) > 1:\n",
    "           new_text += \" \" + w\n",
    "    if len(new_text) > 0 and new_text[0] == \" \":\n",
    "        new_text = new_text[1:]\n",
    "        \n",
    "    return new_text\n",
    "\n",
    "def stem_words(text):\n",
    "    stemmer = nltk.stem.PorterStemmer()\n",
    "    new_text = \"\"\n",
    "    words = text.split()\n",
    "    for w in words:\n",
    "        w = stemmer.stem(w) \n",
    "        new_text += \" \" + w\n",
    "        \n",
    "    if new_text[0] == \" \":\n",
    "        new_text = new_text[1:]\n",
    "        \n",
    "    return new_text\n",
    "\n",
    "def lemmatize_words(text):\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    new_text = \"\"\n",
    "    words = text.split()\n",
    "    for w in words:\n",
    "        w = lemmatizer.lemmatize(w) \n",
    "        new_text += \" \" + w\n",
    "        \n",
    "    if len(new_text) > 0 and new_text[0] == \" \":\n",
    "        new_text = new_text[1:]\n",
    "        \n",
    "    return new_text\n",
    "\n",
    "def remove_stopwords(text, choice):\n",
    "    stopword_list = []\n",
    "    if choice == \"own\":\n",
    "        stopword_list = common_words\n",
    "    elif choice == \"nltk\":\n",
    "        stopword_list = nltk.corpus.stopwords.words(\"english\")\n",
    "    else:\n",
    "        print(\"Choose 'own' or 'nltk'.\")\n",
    "        return text\n",
    "        \n",
    "    new_text = \"\"\n",
    "    words = text.split()\n",
    "    for w in words:\n",
    "        if w not in stopword_list:\n",
    "           new_text += \" \" + w\n",
    "    if len(new_text) > 0 and new_text[0] == \" \":\n",
    "        new_text = new_text[1:]\n",
    "        \n",
    "    return new_text\n",
    "\n",
    "\n",
    "\n",
    "def convert_numbers(data):\n",
    "    tokens = nltk.tokenize.word_tokenize(str(data))\n",
    "    new_text = \"\"\n",
    "    for w in tokens:\n",
    "        try:\n",
    "            w = num2words(int(w))\n",
    "        except:\n",
    "            a = 0\n",
    "        new_text = new_text + \" \" + w\n",
    "    \n",
    "    new_text = np.array_str(np.char.replace(new_text, \"-\", \" \"))\n",
    "                            \n",
    "    return new_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# main preprocessing function\n",
    "def preprocess(text):\n",
    "    text = convert_to_lowercase(text)\n",
    "    text = remove_punctuation(text)\n",
    "    text = remove_apostrophe(text)\n",
    "    text = remove_stopwords(text, \"nltk\")\n",
    "    text = convert_numbers(text)\n",
    "    text = lemmatize_words(text)\n",
    "    text = remove_punctuation(text)\n",
    "    text = convert_numbers(text)\n",
    "    text = lemmatize_words(text)\n",
    "    text = remove_punctuation(text)\n",
    "    text = remove_stopwords(text, \"nltk\")\n",
    "    text = remove_single_characters(text)\n",
    "    \n",
    "    #text = remove_stopwords(text, \"own\")\n",
    "    #text = stem_words(text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "processed_title = {}\n",
    "processed_abstract = {}\n",
    "processed_keywords = {}\n",
    "processed_authors = {}\n",
    "\n",
    "\n",
    "for i in range(1, len(documents)+1):\n",
    "    doc = documents.get(i)\n",
    "    title = doc[\"title\"]\n",
    "    abstract = \"\"\n",
    "    authors = \"\"\n",
    "    keywords = \"\"\n",
    "    \n",
    "    if \"abstract\" in doc:\n",
    "        abstract = doc[\"abstract\"]   \n",
    "    if \"keywords\" in doc:\n",
    "        keywords = doc[\"keywords\"]  \n",
    "    if \"authors\" in doc:\n",
    "        authors = doc[\"authors\"]\n",
    "    \n",
    "    processed_title[i] = nltk.tokenize.word_tokenize(preprocess(title))\n",
    "    processed_abstract[i] = nltk.tokenize.word_tokenize(preprocess(abstract))\n",
    "    processed_keywords[i] = nltk.tokenize.word_tokenize(preprocess(keywords))\n",
    "    processed_authors[i] = nltk.tokenize.word_tokenize(preprocess(authors))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "DF = {}\n",
    "\n",
    "for i in range(1, len(processed_title) + 1):\n",
    "    \n",
    "    tokens = processed_title[i]\n",
    "    for w in tokens:\n",
    "        try:\n",
    "            DF[w].add(i)\n",
    "        except:\n",
    "            DF[w] = {i}\n",
    "\n",
    "    tokens = processed_abstract[i]\n",
    "    \n",
    "    for w in tokens:\n",
    "        try:\n",
    "            DF[w].add(i)\n",
    "        except:\n",
    "            DF[w] = {i}\n",
    "        tokens = processed_keywords[i]\n",
    "        \n",
    "    for w in tokens:\n",
    "        try:\n",
    "            DF[w].add(i)\n",
    "        except:\n",
    "            DF[w] = {i}\n",
    "            \n",
    "        tokens = processed_authors[i]\n",
    "        \n",
    "    for w in tokens:\n",
    "        try:\n",
    "            DF[w].add(i)\n",
    "        except:\n",
    "            DF[w] = {i}\n",
    "            \n",
    "for i in DF:\n",
    "    DF[i] = len(DF[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "total_vocab_size = len(DF)\n",
    "\n",
    "total_vocab = [x for x in DF]\n",
    "\n",
    "N = len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def doc_freq(word):\n",
    "    c = 0\n",
    "    try:\n",
    "        c = DF[word]\n",
    "    except:\n",
    "        pass\n",
    "    return c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF for Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "doc = 1\n",
    "\n",
    "tf_idf_title = {}\n",
    "\n",
    "for i in range(1, N + 1):\n",
    "    \n",
    "    tokens = processed_title[i]\n",
    "    \n",
    "    counter = Counter(tokens + processed_abstract[i] + processed_keywords[i] + processed_authors[i])\n",
    "    words_count = len(tokens + processed_abstract[i] + processed_keywords[i] + processed_authors[i])\n",
    "    \n",
    "    for token in np.unique(tokens):\n",
    "        \n",
    "        tf = counter[token]/words_count\n",
    "        df = doc_freq(token)\n",
    "        idf = np.log((N+1)/(df+1))\n",
    "        \n",
    "        tf_idf_title[doc, token] = tf*idf\n",
    "\n",
    "    doc += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF for Abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "doc = 1\n",
    "\n",
    "tf_idf_abstract = {}\n",
    "\n",
    "for i in range(1, N + 1):\n",
    "    \n",
    "    tokens = processed_abstract[i]\n",
    "    \n",
    "    counter = Counter(tokens + processed_title[i] + processed_keywords[i] + processed_authors[i])\n",
    "    words_count = len(tokens + processed_title[i] + processed_keywords[i] + processed_authors[i])\n",
    "    \n",
    "    for token in np.unique(tokens):\n",
    "        \n",
    "        tf = counter[token]/words_count\n",
    "        df = doc_freq(token)\n",
    "        idf = np.log((N+1)/(df+1))\n",
    "        \n",
    "        tf_idf_abstract[doc, token] = tf*idf\n",
    "\n",
    "    doc += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF for Keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "doc = 1\n",
    "\n",
    "tf_idf_keywords = {}\n",
    "\n",
    "for i in range(1, N + 1):\n",
    "    \n",
    "    tokens = processed_keywords[i]\n",
    "    \n",
    "    counter = Counter(tokens + processed_title[i] + processed_abstract[i] + processed_authors[i])\n",
    "    words_count = len(tokens + processed_title[i] + processed_abstract[i] + processed_authors[i])\n",
    "    \n",
    "    for token in np.unique(tokens):\n",
    "        \n",
    "        tf = counter[token]/words_count\n",
    "        df = doc_freq(token)\n",
    "        idf = np.log((N+1)/(df+1))\n",
    "        \n",
    "        tf_idf_keywords[doc, token] = tf*idf\n",
    "\n",
    "    doc += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF for Authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "doc = 1\n",
    "\n",
    "tf_idf_authors = {}\n",
    "\n",
    "for i in range(1, N + 1):\n",
    "    \n",
    "    tokens = processed_authors[i]\n",
    "    \n",
    "    counter = Counter(tokens + processed_title[i] + processed_abstract[i] + processed_keywords[i])\n",
    "    words_count = len(tokens + processed_title[i] + processed_abstract[i] + processed_keywords[i])\n",
    "    \n",
    "    for token in np.unique(tokens):\n",
    "        \n",
    "        tf = counter[token]/words_count\n",
    "        df = doc_freq(token)\n",
    "        idf = np.log((N+1)/(df+1))\n",
    "        \n",
    "        tf_idf_authors[doc, token] = tf*idf\n",
    "\n",
    "    doc += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matching Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matching_score(k, queryNum):\n",
    "    query = queries[queryNum]\n",
    "    preprocessed_query = preprocess(query)\n",
    "    tokens = nltk.tokenize.word_tokenize(str(preprocessed_query))\n",
    "    \n",
    "    query_weights = {}\n",
    "\n",
    "    for key in tf_idf_title:\n",
    "        \n",
    "        if key[1] in tokens:\n",
    "            try:\n",
    "                query_weights[key[0]] += tf_idf_title[key]\n",
    "            except:\n",
    "                query_weights[key[0]] = tf_idf_title[key]\n",
    "    \n",
    "    query_weights = sorted(query_weights.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    l = []\n",
    "    \n",
    "    for i in query_weights[:k]:\n",
    "        l.append(i[0])\n",
    "    \n",
    "    show = False\n",
    "    for item in l:\n",
    "        if queryNum in queryRelations and item in queryRelations[queryNum]:\n",
    "            show = True\n",
    "    \n",
    "    if show:\n",
    "        #print(\"Query:\", query)\n",
    "        #print(\"Predictions:\", l)\n",
    "        #print(\"Actual:\", queryRelations[queryNum])\n",
    "        #print(\"\")\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67.3 %\n"
     ]
    }
   ],
   "source": [
    "k = 20\n",
    "counter = 0\n",
    "for n in queries:\n",
    "    counter += matching_score(k, n)\n",
    "print(round((counter / len(queryRelations)) * 100, 1), \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
