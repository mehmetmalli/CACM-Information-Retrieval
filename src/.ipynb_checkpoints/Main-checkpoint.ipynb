{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "!pip3 install num2words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "import math\n",
    "from num2words import num2words\n",
    "from collections import Counter\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# read all data file\n",
    "filename = \"../files/cacm.all_processed\"\n",
    "\n",
    "with open(filename, 'r') as file:\n",
    "    linesArr = file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# remove newlines\n",
    "for i in range(len(linesArr)):\n",
    "    linesArr[i] = linesArr[i].replace(\"\\n\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# create array for each document\n",
    "documentsArr = []\n",
    "document = []\n",
    "for i in range(len(linesArr)):\n",
    "    line = linesArr[i]\n",
    "    if(line == \"SPLITTER_TEXT\"):\n",
    "        if(i != 0):\n",
    "            documentsArr.append(document)    \n",
    "        document = []\n",
    "    else:\n",
    "        document.append(line)\n",
    "        \n",
    "if(len(documentsArr) == 3204):\n",
    "    print(\"All documents have been successfully seperated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "documents = {}\n",
    "\n",
    "for document in documentsArr:\n",
    "    docObj = {}\n",
    "    \n",
    "    docID = int(document[0].split(\" \")[1])\n",
    "    \n",
    "    title = \"\"\n",
    "    \n",
    "    indexofT = document.index(\".T\")\n",
    "    indexofB = document.index(\".B\")\n",
    "    \n",
    "    if \".W\" in document:\n",
    "        abstract = \"\"\n",
    "        indexofW = document.index(\".W\")\n",
    "        for i in range(indexofT + 1, indexofW):\n",
    "            title += \" \" + document[i]\n",
    "        for i in range(indexofW + 1, indexofB):\n",
    "            abstract += \" \" + document[i]\n",
    "        docObj[\"abstract\"] = abstract[1:]\n",
    "        \n",
    "    else:\n",
    "        for i in range(indexofT + 1, indexofB):\n",
    "            title += \" \" + document[i]\n",
    "            \n",
    "    docObj[\"title\"] = title[1:]        \n",
    "            \n",
    "    if \".A\" in document:\n",
    "        authors = \"\"\n",
    "        indexofA = document.index(\".A\")\n",
    "        \n",
    "        if \".K\" in document:\n",
    "            indexofK = document.index(\".K\")\n",
    "            for i in range(indexofA + 1, indexofK):\n",
    "                authors += \" \" + document[i]\n",
    "                \n",
    "        else:\n",
    "            for i in range(indexofA + 1, len(document)):\n",
    "                authors += \" \" + document[i]\n",
    "            \n",
    "        docObj[\"authors\"] = authors[1:]\n",
    "        \n",
    "    if \".K\" in document:\n",
    "        keywords = \"\"\n",
    "        indexofK = document.index(\".K\")\n",
    "        for i in range(indexofK + 1, len(document)):\n",
    "            keywords += \" \" + document[i]\n",
    "        docObj[\"keywords\"] = keywords[1:]\n",
    "    \n",
    "\n",
    "    \n",
    "    documents[docID] = docObj\n",
    "\n",
    "if(len(documents) == 3204):\n",
    "    print(\"All documents have been successfully parsed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# read queries file\n",
    "filename = \"../files/query.text\"\n",
    "\n",
    "with open(filename, 'r') as file:\n",
    "    linesArr = file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# remove newlines\n",
    "for i in range(len(linesArr)):\n",
    "    linesArr[i] = linesArr[i].replace(\"\\n\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# create array for each query\n",
    "queriesArr = []\n",
    "query = []\n",
    "for i in range(len(linesArr)):\n",
    "    line = linesArr[i]\n",
    "    if(line == \" \" or line == \"\"):\n",
    "        if(i != 0):\n",
    "            queriesArr.append(query)    \n",
    "        query = []\n",
    "    else:\n",
    "        query.append(line)\n",
    "        \n",
    "if(len(queriesArr) == 64):\n",
    "    print(\"All queries have been successfully seperated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# additional parsing and organization of queries\n",
    "queries = {}\n",
    "\n",
    "for query in queriesArr:\n",
    "    queryText = \"\"\n",
    "    queryID = int(query[0].split(\" \")[1])\n",
    "    indexofW = query.index(\".W\")\n",
    "    if \".A\" in query:\n",
    "        indexofA = query.index(\".A\")\n",
    "        for i in range(indexofW + 1, indexofA):\n",
    "            queryText += \" \" + query[i]\n",
    "    else:\n",
    "        indexofN = query.index(\".N\")\n",
    "        for i in range(indexofW + 1, indexofN):\n",
    "            queryText += \" \" + query[i]\n",
    "    queries[queryID] = queryText[2:]\n",
    "    \n",
    "if(len(queries) == 64):\n",
    "    print(\"All queries have been successfully parsed.\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# read query relations file\n",
    "filename = \"../files/qrels.text\"\n",
    "\n",
    "with open(filename, 'r') as file:\n",
    "    linesArr = file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# create dictionary to store query relations\n",
    "queryRelations = {}\n",
    "\n",
    "for line in linesArr:\n",
    "    line = line.split(\" \")\n",
    "    queryID = int(line[0])\n",
    "    docID = int(line[1])\n",
    "    \n",
    "    if queryID in queryRelations:\n",
    "        temp = queryRelations[queryID]\n",
    "        temp.append(docID)\n",
    "        queryRelations[queryID] = temp\n",
    "    else:\n",
    "        queryRelations[queryID] = [docID]\n",
    "\n",
    "if(len(queryRelations) == 52):\n",
    "    print(\"All query relations have been successfully parsed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# read query relations file\n",
    "filename = \"../files/common_words\"\n",
    "\n",
    "with open(filename, 'r') as file:\n",
    "    common_words = file.readlines()\n",
    "\n",
    "if len(common_words) == 429:\n",
    "    print(\"All common words have been successfully parsed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# define sub-preprocessing functions\n",
    "def convert_to_lowercase(text):\n",
    "    return np.array_str(np.char.lower(text))\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    symbols = \"!\\\"#$%&()*+-./:;,<=>?@[\\]^_`{|}~\\n\"\n",
    "    for i in symbols:\n",
    "        text = np.array_str(np.char.replace(text, i, ' '))\n",
    "    return text  \n",
    "\n",
    "def remove_apostrophe(text):\n",
    "    return np.array_str(np.char.replace(text, \"'\", \"\"))\n",
    "\n",
    "def remove_single_characters(text):\n",
    "    new_text = \"\"\n",
    "    words = text.split()\n",
    "    for w in words:\n",
    "        if len(w) > 1:\n",
    "           new_text += \" \" + w\n",
    "    if len(new_text) > 0 and new_text[0] == \" \":\n",
    "        new_text = new_text[1:]\n",
    "        \n",
    "    return new_text\n",
    "\n",
    "def stem_words(text):\n",
    "    stemmer = nltk.stem.PorterStemmer()\n",
    "    new_text = \"\"\n",
    "    words = text.split()\n",
    "    for w in words:\n",
    "        w = stemmer.stem(w) \n",
    "        new_text += \" \" + w\n",
    "        \n",
    "    if new_text[0] == \" \":\n",
    "        new_text = new_text[1:]\n",
    "        \n",
    "    return new_text\n",
    "\n",
    "def lemmatize_words(text):\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    new_text = \"\"\n",
    "    words = text.split()\n",
    "    for w in words:\n",
    "        w = lemmatizer.lemmatize(w) \n",
    "        new_text += \" \" + w\n",
    "        \n",
    "    if len(new_text) > 0 and new_text[0] == \" \":\n",
    "        new_text = new_text[1:]\n",
    "        \n",
    "    return new_text\n",
    "\n",
    "def remove_stopwords(text, choice):\n",
    "    stopword_list = []\n",
    "    if choice == \"own\":\n",
    "        stopword_list = common_words\n",
    "    elif choice == \"nltk\":\n",
    "        stopword_list = nltk.corpus.stopwords.words(\"english\")\n",
    "    else:\n",
    "        print(\"Choose 'own' or 'nltk'.\")\n",
    "        return text\n",
    "        \n",
    "    new_text = \"\"\n",
    "    words = text.split()\n",
    "    for w in words:\n",
    "        if w not in stopword_list:\n",
    "           new_text += \" \" + w\n",
    "    if len(new_text) > 0 and new_text[0] == \" \":\n",
    "        new_text = new_text[1:]\n",
    "        \n",
    "    return new_text\n",
    "\n",
    "\n",
    "\n",
    "def convert_numbers(data):\n",
    "    tokens = nltk.tokenize.word_tokenize(str(data))\n",
    "    new_text = \"\"\n",
    "    for w in tokens:\n",
    "        try:\n",
    "            w = num2words(int(w))\n",
    "        except:\n",
    "            a = 0\n",
    "        new_text = new_text + \" \" + w\n",
    "    \n",
    "    new_text = np.array_str(np.char.replace(new_text, \"-\", \" \"))\n",
    "                            \n",
    "    return new_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# main preprocessing function\n",
    "def preprocess(text):\n",
    "    text = convert_to_lowercase(text)\n",
    "    text = remove_punctuation(text)\n",
    "    text = remove_apostrophe(text)\n",
    "    text = remove_stopwords(text, \"nltk\")\n",
    "    text = convert_numbers(text)\n",
    "    text = lemmatize_words(text)\n",
    "    text = remove_punctuation(text)\n",
    "    text = convert_numbers(text)\n",
    "    text = lemmatize_words(text)\n",
    "    text = remove_punctuation(text)\n",
    "    text = remove_stopwords(text, \"nltk\")\n",
    "    text = remove_single_characters(text)\n",
    "    \n",
    "    #text = remove_stopwords(text, \"own\")\n",
    "    #text = stem_words(text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "processed_title = {}\n",
    "processed_abstract = {}\n",
    "processed_keywords = {}\n",
    "processed_authors = {}\n",
    "\n",
    "\n",
    "for i in range(1, len(documents)+1):\n",
    "    doc = documents.get(i)\n",
    "    title = doc[\"title\"]\n",
    "    abstract = \"\"\n",
    "    authors = \"\"\n",
    "    keywords = \"\"\n",
    "    \n",
    "    if \"abstract\" in doc:\n",
    "        abstract = doc[\"abstract\"]   \n",
    "    if \"keywords\" in doc:\n",
    "        keywords = doc[\"keywords\"]  \n",
    "    if \"authors\" in doc:\n",
    "        authors = doc[\"authors\"]\n",
    "    \n",
    "    processed_title[i] = nltk.tokenize.word_tokenize(preprocess(title))\n",
    "    processed_abstract[i] = nltk.tokenize.word_tokenize(preprocess(abstract))\n",
    "    processed_keywords[i] = nltk.tokenize.word_tokenize(preprocess(keywords))\n",
    "    processed_authors[i] = nltk.tokenize.word_tokenize(preprocess(authors))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "DF = {}\n",
    "\n",
    "for i in range(1, len(processed_title) + 1):\n",
    "    \n",
    "    tokens = processed_title[i]\n",
    "    for w in tokens:\n",
    "        try:\n",
    "            DF[w].add(i)\n",
    "        except:\n",
    "            DF[w] = {i}\n",
    "\n",
    "    tokens = processed_abstract[i]\n",
    "    \n",
    "    for w in tokens:\n",
    "        try:\n",
    "            DF[w].add(i)\n",
    "        except:\n",
    "            DF[w] = {i}\n",
    "        tokens = processed_keywords[i]\n",
    "        \n",
    "    for w in tokens:\n",
    "        try:\n",
    "            DF[w].add(i)\n",
    "        except:\n",
    "            DF[w] = {i}\n",
    "            \n",
    "        tokens = processed_authors[i]\n",
    "        \n",
    "    for w in tokens:\n",
    "        try:\n",
    "            DF[w].add(i)\n",
    "        except:\n",
    "            DF[w] = {i}\n",
    "            \n",
    "for i in DF:\n",
    "    DF[i] = len(DF[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "total_vocab_size = len(DF)\n",
    "\n",
    "total_vocab = [x for x in DF]\n",
    "\n",
    "N = len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def doc_freq(word):\n",
    "    c = 0\n",
    "    try:\n",
    "        c = DF[word]\n",
    "    except:\n",
    "        pass\n",
    "    return c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF for Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "doc = 1\n",
    "\n",
    "tf_idf_title = {}\n",
    "\n",
    "for i in range(1, N + 1):\n",
    "    \n",
    "    tokens = processed_title[i]\n",
    "    \n",
    "    counter = Counter(tokens + processed_abstract[i] + processed_keywords[i] + processed_authors[i])\n",
    "    words_count = len(tokens + processed_abstract[i] + processed_keywords[i] + processed_authors[i])\n",
    "    \n",
    "    for token in np.unique(tokens):\n",
    "        \n",
    "        tf = counter[token]/words_count\n",
    "        df = doc_freq(token)\n",
    "        idf = np.log((N+1)/(df+1))\n",
    "        \n",
    "        tf_idf_title[doc, token] = tf*idf\n",
    "\n",
    "    doc += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF for Abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "doc = 1\n",
    "\n",
    "tf_idf_abstract = {}\n",
    "\n",
    "for i in range(1, N + 1):\n",
    "    \n",
    "    tokens = processed_abstract[i]\n",
    "    \n",
    "    counter = Counter(tokens + processed_title[i] + processed_keywords[i] + processed_authors[i])\n",
    "    words_count = len(tokens + processed_title[i] + processed_keywords[i] + processed_authors[i])\n",
    "    \n",
    "    for token in np.unique(tokens):\n",
    "        \n",
    "        tf = counter[token]/words_count\n",
    "        df = doc_freq(token)\n",
    "        idf = np.log((N+1)/(df+1))\n",
    "        \n",
    "        tf_idf_abstract[doc, token] = tf*idf\n",
    "\n",
    "    doc += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF for Keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "doc = 1\n",
    "\n",
    "tf_idf_keywords = {}\n",
    "\n",
    "for i in range(1, N + 1):\n",
    "    \n",
    "    tokens = processed_keywords[i]\n",
    "    \n",
    "    counter = Counter(tokens + processed_title[i] + processed_abstract[i] + processed_authors[i])\n",
    "    words_count = len(tokens + processed_title[i] + processed_abstract[i] + processed_authors[i])\n",
    "    \n",
    "    for token in np.unique(tokens):\n",
    "        \n",
    "        tf = counter[token]/words_count\n",
    "        df = doc_freq(token)\n",
    "        idf = np.log((N+1)/(df+1))\n",
    "        \n",
    "        tf_idf_keywords[doc, token] = tf*idf\n",
    "\n",
    "    doc += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF for Authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "doc = 1\n",
    "\n",
    "tf_idf_authors = {}\n",
    "\n",
    "for i in range(1, N + 1):\n",
    "    \n",
    "    tokens = processed_authors[i]\n",
    "    \n",
    "    counter = Counter(tokens + processed_title[i] + processed_abstract[i] + processed_keywords[i])\n",
    "    words_count = len(tokens + processed_title[i] + processed_abstract[i] + processed_keywords[i])\n",
    "    \n",
    "    for token in np.unique(tokens):\n",
    "        \n",
    "        tf = counter[token]/words_count\n",
    "        df = doc_freq(token)\n",
    "        idf = np.log((N+1)/(df+1))\n",
    "        \n",
    "        tf_idf_authors[doc, token] = tf*idf\n",
    "\n",
    "    doc += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# TODO ??\n",
    "\n",
    "for i in tf_idf_title:\n",
    "    tf_idf_title[i] *= 0.5\n",
    "    \n",
    "for i in tf_idf_abstract:\n",
    "    tf_idf_title[i] = tf_idf_abstract[i]\n",
    "    \n",
    "for i in tf_idf_keywords:\n",
    "    tf_idf_title[i] = tf_idf_keywords[i]\n",
    "\n",
    "for i in tf_idf_authors:\n",
    "    tf_idf_title[i] = tf_idf_authors[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matching Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def matching_score(k, queryNum):\n",
    "    query = queries[queryNum]\n",
    "    preprocessed_query = preprocess(query)\n",
    "    tokens = nltk.tokenize.word_tokenize(str(preprocessed_query))\n",
    "    match_counter = 0\n",
    "    query_weights = {}\n",
    "\n",
    "    for key in tf_idf_title:\n",
    "        \n",
    "        if key[1] in tokens:\n",
    "            try:\n",
    "                query_weights[key[0]] += tf_idf_title[key]\n",
    "            except:\n",
    "                query_weights[key[0]] = tf_idf_title[key]\n",
    "    \n",
    "    query_weights = sorted(query_weights.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    l = []\n",
    "    \n",
    "    for i in query_weights[:k]:\n",
    "        l.append(i[0])\n",
    "    \n",
    "    show = False\n",
    "    for item in l:\n",
    "        if queryNum in queryRelations and item in queryRelations[queryNum]:\n",
    "            show = True\n",
    "            match_counter += 1\n",
    "            \n",
    "            \n",
    "    if queryNum in queryRelations:\n",
    "        recall = str(round(match_counter/len(queryRelations[queryNum]), 2)*100) + \" %\"\n",
    "    else:\n",
    "        recall = \"No matching docs\"\n",
    "        \n",
    "    \n",
    "    print(\"Q:\", queryNum, \"Precision:\", round((match_counter/k), 2)*100, \"%\" , \"Recall:\", recall)\n",
    "    if show:\n",
    "        #print(\"Query:\", query)\n",
    "        #print(\"Predictions:\", l)\n",
    "        #print(\"Actual:\", queryRelations[queryNum])\n",
    "        #print(\"Match count: \", match_counter)\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: 1 Precision: 10.0 % Recall: 40.0 %\n",
      "Q: 2 Precision: 5.0 % Recall: 33.0 %\n",
      "Q: 3 Precision: 0.0 % Recall: 0.0 %\n",
      "Q: 4 Precision: 15.0 % Recall: 25.0 %\n",
      "Q: 5 Precision: 20.0 % Recall: 50.0 %\n",
      "Q: 6 Precision: 5.0 % Recall: 33.0 %\n",
      "Q: 7 Precision: 30.0 % Recall: 21.0 %\n",
      "Q: 8 Precision: 5.0 % Recall: 33.0 %\n",
      "Q: 9 Precision: 15.0 % Recall: 33.0 %\n",
      "Q: 10 Precision: 45.0 % Recall: 26.0 %\n",
      "Q: 11 Precision: 40.0 % Recall: 42.0 %\n",
      "Q: 12 Precision: 15.0 % Recall: 60.0 %\n",
      "Q: 13 Precision: 25.0 % Recall: 45.0 %\n",
      "Q: 14 Precision: 25.0 % Recall: 11.0 %\n",
      "Q: 15 Precision: 15.0 % Recall: 30.0 %\n",
      "Q: 16 Precision: 15.0 % Recall: 18.0 %\n",
      "Q: 17 Precision: 10.0 % Recall: 12.0 %\n",
      "Q: 18 Precision: 5.0 % Recall: 9.0 %\n",
      "Q: 19 Precision: 40.0 % Recall: 73.0 %\n",
      "Q: 20 Precision: 0.0 % Recall: 0.0 %\n",
      "Q: 21 Precision: 15.0 % Recall: 27.0 %\n",
      "Q: 22 Precision: 50.0 % Recall: 59.0 %\n",
      "Q: 23 Precision: 20.0 % Recall: 100.0 %\n",
      "Q: 24 Precision: 10.0 % Recall: 15.0 %\n",
      "Q: 25 Precision: 40.0 % Recall: 16.0 %\n",
      "Q: 26 Precision: 20.0 % Recall: 13.0 %\n",
      "Q: 27 Precision: 40.0 % Recall: 28.000000000000004 %\n",
      "Q: 28 Precision: 20.0 % Recall: 80.0 %\n",
      "Q: 29 Precision: 55.00000000000001 % Recall: 57.99999999999999 %\n",
      "Q: 30 Precision: 15.0 % Recall: 75.0 %\n",
      "Q: 31 Precision: 5.0 % Recall: 50.0 %\n",
      "Q: 32 Precision: 5.0 % Recall: 33.0 %\n",
      "Q: 33 Precision: 0.0 % Recall: 0.0 %\n",
      "Q: 34 Precision: 0.0 % Recall: No matching docs\n",
      "Q: 35 Precision: 0.0 % Recall: No matching docs\n",
      "Q: 36 Precision: 25.0 % Recall: 25.0 %\n",
      "Q: 37 Precision: 10.0 % Recall: 17.0 %\n",
      "Q: 38 Precision: 30.0 % Recall: 38.0 %\n",
      "Q: 39 Precision: 20.0 % Recall: 33.0 %\n",
      "Q: 40 Precision: 15.0 % Recall: 30.0 %\n",
      "Q: 41 Precision: 0.0 % Recall: No matching docs\n",
      "Q: 42 Precision: 15.0 % Recall: 14.000000000000002 %\n",
      "Q: 43 Precision: 15.0 % Recall: 7.000000000000001 %\n",
      "Q: 44 Precision: 10.0 % Recall: 12.0 %\n",
      "Q: 45 Precision: 35.0 % Recall: 27.0 %\n",
      "Q: 46 Precision: 0.0 % Recall: No matching docs\n",
      "Q: 47 Precision: 0.0 % Recall: No matching docs\n",
      "Q: 48 Precision: 25.0 % Recall: 42.0 %\n",
      "Q: 49 Precision: 0.0 % Recall: 0.0 %\n",
      "Q: 50 Precision: 0.0 % Recall: No matching docs\n",
      "Q: 51 Precision: 0.0 % Recall: No matching docs\n",
      "Q: 52 Precision: 0.0 % Recall: No matching docs\n",
      "Q: 53 Precision: 0.0 % Recall: No matching docs\n",
      "Q: 54 Precision: 0.0 % Recall: No matching docs\n",
      "Q: 55 Precision: 0.0 % Recall: No matching docs\n",
      "Q: 56 Precision: 0.0 % Recall: No matching docs\n",
      "Q: 57 Precision: 5.0 % Recall: 100.0 %\n",
      "Q: 58 Precision: 20.0 % Recall: 13.0 %\n",
      "Q: 59 Precision: 35.0 % Recall: 16.0 %\n",
      "Q: 60 Precision: 20.0 % Recall: 15.0 %\n",
      "Q: 61 Precision: 55.00000000000001 % Recall: 35.0 %\n",
      "Q: 62 Precision: 5.0 % Recall: 12.0 %\n",
      "Q: 63 Precision: 20.0 % Recall: 33.0 %\n",
      "Q: 64 Precision: 0.0 % Recall: 0.0 %\n",
      "90.4 %\n"
     ]
    }
   ],
   "source": [
    "k = 20\n",
    "counter = 0\n",
    "for n in queries:\n",
    "    counter += matching_score(k, n)\n",
    "print(round((counter / len(queryRelations)) * 100, 1), \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def cosine_sim(a, b):\n",
    "    cos_sim = np.dot(a, b)/(np.linalg.norm(a)*np.linalg.norm(b))\n",
    "    return cos_sim\n",
    "\n",
    "D = np.zeros((N, total_vocab_size))\n",
    "for i in tf_idf_title:\n",
    "    try:\n",
    "        ind = total_vocab.index(i[1])\n",
    "        D[i[0]][ind] = tf_idf_title[i]\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "\n",
    "\n",
    "def gen_vector(tokens):\n",
    "\n",
    "    Q = np.zeros((len(total_vocab)))\n",
    "    \n",
    "    counter = Counter(tokens)\n",
    "    words_count = len(tokens)\n",
    "\n",
    "    query_weights = {}\n",
    "    \n",
    "    for token in np.unique(tokens):\n",
    "        \n",
    "        tf = counter[token]/words_count\n",
    "        df = doc_freq(token)\n",
    "        idf = math.log((N+1)/(df+1))\n",
    "\n",
    "        try:\n",
    "            ind = total_vocab.index(token)\n",
    "            Q[ind] = tf*idf\n",
    "        except:\n",
    "            pass\n",
    "    return Q\n",
    "\n",
    "\n",
    "def cosine_similarity(k, queryNum):\n",
    "    query = queries[queryNum]\n",
    "    preprocessed_query = preprocess(query)\n",
    "    tokens = nltk.tokenize.word_tokenize(str(preprocessed_query))\n",
    "    match_counter = 0\n",
    "    \n",
    "    d_cosines = []\n",
    "    \n",
    "    query_vector = gen_vector(tokens)\n",
    "    \n",
    "    for d in D:\n",
    "        d_cosines.append(cosine_sim(query_vector, d))\n",
    "        \n",
    "    out = np.array(d_cosines).argsort()[-k-1:][::-1]\n",
    "    \n",
    "    out = np.delete(out, 0)\n",
    "    \n",
    "    show = False\n",
    "    for item in out:\n",
    "        if queryNum in queryRelations and item in queryRelations[queryNum]:\n",
    "            show = True\n",
    "            match_counter += 1\n",
    "            \n",
    "    if queryNum in queryRelations:\n",
    "        recall = str(round(match_counter/len(queryRelations[queryNum]), 2)*100) + \" %\"\n",
    "    else:\n",
    "        recall = \"No matching docs\"\n",
    "    \n",
    "    print(\"Q:\", queryNum, \"Precision:\", round((match_counter/k), 2)*100, \"%\" , \"Recall:\", recall)\n",
    "    \n",
    "    if show:\n",
    "        #print(\"Query:\", query)\n",
    "        #print(\"Predictions:\", out)\n",
    "        #print(\"Actual:\", queryRelations[queryNum])\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-238-d4138188abea>:2: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  cos_sim = np.dot(a, b)/(np.linalg.norm(a)*np.linalg.norm(b))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: 1 Precision: 15.0 % Recall: 60.0 %\n",
      "Q: 2 Precision: 15.0 % Recall: 100.0 %\n",
      "Q: 3 Precision: 5.0 % Recall: 17.0 %\n",
      "Q: 4 Precision: 15.0 % Recall: 25.0 %\n",
      "Q: 5 Precision: 15.0 % Recall: 38.0 %\n",
      "Q: 6 Precision: 15.0 % Recall: 100.0 %\n",
      "Q: 7 Precision: 40.0 % Recall: 28.999999999999996 %\n",
      "Q: 8 Precision: 5.0 % Recall: 33.0 %\n",
      "Q: 9 Precision: 15.0 % Recall: 33.0 %\n",
      "Q: 10 Precision: 80.0 % Recall: 46.0 %\n",
      "Q: 11 Precision: 45.0 % Recall: 47.0 %\n",
      "Q: 12 Precision: 15.0 % Recall: 60.0 %\n",
      "Q: 13 Precision: 25.0 % Recall: 45.0 %\n",
      "Q: 14 Precision: 25.0 % Recall: 11.0 %\n",
      "Q: 15 Precision: 20.0 % Recall: 40.0 %\n",
      "Q: 16 Precision: 15.0 % Recall: 18.0 %\n",
      "Q: 17 Precision: 20.0 % Recall: 25.0 %\n",
      "Q: 18 Precision: 10.0 % Recall: 18.0 %\n",
      "Q: 19 Precision: 40.0 % Recall: 73.0 %\n",
      "Q: 20 Precision: 15.0 % Recall: 100.0 %\n",
      "Q: 21 Precision: 15.0 % Recall: 27.0 %\n",
      "Q: 22 Precision: 55.00000000000001 % Recall: 65.0 %\n",
      "Q: 23 Precision: 20.0 % Recall: 100.0 %\n",
      "Q: 24 Precision: 10.0 % Recall: 15.0 %\n",
      "Q: 25 Precision: 35.0 % Recall: 14.000000000000002 %\n",
      "Q: 26 Precision: 25.0 % Recall: 17.0 %\n",
      "Q: 27 Precision: 45.0 % Recall: 31.0 %\n",
      "Q: 28 Precision: 20.0 % Recall: 80.0 %\n",
      "Q: 29 Precision: 70.0 % Recall: 74.0 %\n",
      "Q: 30 Precision: 10.0 % Recall: 50.0 %\n",
      "Q: 31 Precision: 10.0 % Recall: 100.0 %\n",
      "Q: 32 Precision: 10.0 % Recall: 67.0 %\n",
      "Q: 33 Precision: 0.0 % Recall: 0.0 %\n",
      "Q: 34 Precision: 0.0 % Recall: No matching docs\n",
      "Q: 35 Precision: 0.0 % Recall: No matching docs\n",
      "Q: 36 Precision: 35.0 % Recall: 35.0 %\n",
      "Q: 37 Precision: 20.0 % Recall: 33.0 %\n",
      "Q: 38 Precision: 40.0 % Recall: 50.0 %\n",
      "Q: 39 Precision: 25.0 % Recall: 42.0 %\n",
      "Q: 40 Precision: 25.0 % Recall: 50.0 %\n",
      "Q: 41 Precision: 0.0 % Recall: No matching docs\n",
      "Q: 42 Precision: 15.0 % Recall: 14.000000000000002 %\n",
      "Q: 43 Precision: 35.0 % Recall: 17.0 %\n",
      "Q: 44 Precision: 15.0 % Recall: 18.0 %\n",
      "Q: 45 Precision: 50.0 % Recall: 38.0 %\n",
      "Q: 46 Precision: 0.0 % Recall: No matching docs\n",
      "Q: 47 Precision: 0.0 % Recall: No matching docs\n",
      "Q: 48 Precision: 10.0 % Recall: 17.0 %\n",
      "Q: 49 Precision: 20.0 % Recall: 50.0 %\n",
      "Q: 50 Precision: 0.0 % Recall: No matching docs\n",
      "Q: 51 Precision: 0.0 % Recall: No matching docs\n",
      "Q: 52 Precision: 0.0 % Recall: No matching docs\n",
      "Q: 53 Precision: 0.0 % Recall: No matching docs\n",
      "Q: 54 Precision: 0.0 % Recall: No matching docs\n",
      "Q: 55 Precision: 0.0 % Recall: No matching docs\n",
      "Q: 56 Precision: 0.0 % Recall: No matching docs\n",
      "Q: 57 Precision: 5.0 % Recall: 100.0 %\n",
      "Q: 58 Precision: 45.0 % Recall: 30.0 %\n",
      "Q: 59 Precision: 55.00000000000001 % Recall: 26.0 %\n",
      "Q: 60 Precision: 60.0 % Recall: 44.0 %\n",
      "Q: 61 Precision: 60.0 % Recall: 39.0 %\n",
      "Q: 62 Precision: 5.0 % Recall: 12.0 %\n",
      "Q: 63 Precision: 35.0 % Recall: 57.99999999999999 %\n",
      "Q: 64 Precision: 5.0 % Recall: 100.0 %\n"
     ]
    }
   ],
   "source": [
    "k = 20\n",
    "counter = 0\n",
    "for n in queries:\n",
    "    counter += cosine_similarity(k, n)\n",
    "#print(round((counter / len(queryRelations)) * 100, 1), \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
